{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "class Network1(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "        \n",
    "    def feedforword(self, a):\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k + mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "        \n",
    "\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w - (eta / len(mini_batch)) * nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(mini_batch)) * nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]  # list to store all the activations, layer by layer\n",
    "        zs = []  # list to store all the z vectors, layer by layer\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # backward pass \n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "                sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta \n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
    "        \n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self,test_data):\n",
    "        \n",
    "        test_result = [(np.argmax(self.feedforword(x)), y)\n",
    "                      for (x,y) in test_data]\n",
    "        return sum(int(x==y) for (x,y) in test_result)\n",
    "    \n",
    "    def cost_derivative(self, output_activations,y):\n",
    "        return (output_activations - y)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# mnist_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %load mnist_loader.py\n",
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "`tr_d, va_d, te_d = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "training_data = list(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = Network1([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 9031 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 1, 10, 3.0, test_data=test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9145 / 10000\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9273 / 10000\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9349 / 10000\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9394 / 10000\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9417 / 10000\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9467 / 10000\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9465 / 10000\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9454 / 10000\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9488 / 10000\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9459 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [9145, 9273, 9349, 9394, 9417, 9467, 9465, 9454, 9488, 9459], [], [])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "import network2\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 10, 10, 0.5, evaluation_data=test_data,\n",
    "        monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 1203.0090214904178\n",
      "Accuracy on training data: 638 / 1000\n",
      "Cost on evaluation data: 1203.4062991530584\n",
      "Accuracy on evaluation data: 5329 / 10000\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1192.4582025147165\n",
      "Accuracy on training data: 723 / 1000\n",
      "Cost on evaluation data: 1192.985413664471\n",
      "Accuracy on evaluation data: 5936 / 10000\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1182.3362117293307\n",
      "Accuracy on training data: 837 / 1000\n",
      "Cost on evaluation data: 1182.9081248020473\n",
      "Accuracy on evaluation data: 6975 / 10000\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1172.7878897212183\n",
      "Accuracy on training data: 867 / 1000\n",
      "Cost on evaluation data: 1173.4323235306697\n",
      "Accuracy on evaluation data: 7213 / 10000\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1163.543130540585\n",
      "Accuracy on training data: 904 / 1000\n",
      "Cost on evaluation data: 1164.2212612819574\n",
      "Accuracy on evaluation data: 7466 / 10000\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1154.457855789429\n",
      "Accuracy on training data: 932 / 1000\n",
      "Cost on evaluation data: 1155.165712549201\n",
      "Accuracy on evaluation data: 7639 / 10000\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1145.48987008091\n",
      "Accuracy on training data: 940 / 1000\n",
      "Cost on evaluation data: 1146.2867036503221\n",
      "Accuracy on evaluation data: 7585 / 10000\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1136.4763475310592\n",
      "Accuracy on training data: 947 / 1000\n",
      "Cost on evaluation data: 1137.2833110812892\n",
      "Accuracy on evaluation data: 7741 / 10000\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1127.4506072937318\n",
      "Accuracy on training data: 955 / 1000\n",
      "Cost on evaluation data: 1128.2588473238518\n",
      "Accuracy on evaluation data: 7810 / 10000\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1118.5301572298756\n",
      "Accuracy on training data: 961 / 1000\n",
      "Cost on evaluation data: 1119.4047314673658\n",
      "Accuracy on evaluation data: 7815 / 10000\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 1109.6540165233446\n",
      "Accuracy on training data: 963 / 1000\n",
      "Cost on evaluation data: 1110.5355101167672\n",
      "Accuracy on evaluation data: 7944 / 10000\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 1100.7226382869826\n",
      "Accuracy on training data: 966 / 1000\n",
      "Cost on evaluation data: 1101.609957639381\n",
      "Accuracy on evaluation data: 7949 / 10000\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 1091.898488197894\n",
      "Accuracy on training data: 973 / 1000\n",
      "Cost on evaluation data: 1092.8270754089729\n",
      "Accuracy on evaluation data: 7950 / 10000\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 1083.0925065382703\n",
      "Accuracy on training data: 976 / 1000\n",
      "Cost on evaluation data: 1084.0371431365063\n",
      "Accuracy on evaluation data: 7965 / 10000\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 1074.3047102031946\n",
      "Accuracy on training data: 976 / 1000\n",
      "Cost on evaluation data: 1075.2526908229027\n",
      "Accuracy on evaluation data: 7980 / 10000\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 1065.6137192106235\n",
      "Accuracy on training data: 981 / 1000\n",
      "Cost on evaluation data: 1066.5870980774855\n",
      "Accuracy on evaluation data: 8002 / 10000\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 1056.9217263347111\n",
      "Accuracy on training data: 982 / 1000\n",
      "Cost on evaluation data: 1057.889510794536\n",
      "Accuracy on evaluation data: 8066 / 10000\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 1048.2854082230042\n",
      "Accuracy on training data: 983 / 1000\n",
      "Cost on evaluation data: 1049.2802243071455\n",
      "Accuracy on evaluation data: 8079 / 10000\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 1039.6791453171982\n",
      "Accuracy on training data: 983 / 1000\n",
      "Cost on evaluation data: 1040.6797922046705\n",
      "Accuracy on evaluation data: 8073 / 10000\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 1031.0915281055509\n",
      "Accuracy on training data: 984 / 1000\n",
      "Cost on evaluation data: 1032.0985690518812\n",
      "Accuracy on evaluation data: 8114 / 10000\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 1022.5742678820674\n",
      "Accuracy on training data: 987 / 1000\n",
      "Cost on evaluation data: 1023.6026270906094\n",
      "Accuracy on evaluation data: 8109 / 10000\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 1014.0890842007944\n",
      "Accuracy on training data: 988 / 1000\n",
      "Cost on evaluation data: 1015.120677864256\n",
      "Accuracy on evaluation data: 8151 / 10000\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 1005.6748902372358\n",
      "Accuracy on training data: 990 / 1000\n",
      "Cost on evaluation data: 1006.7063537636782\n",
      "Accuracy on evaluation data: 8159 / 10000\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 997.324754855575\n",
      "Accuracy on training data: 991 / 1000\n",
      "Cost on evaluation data: 998.3809419712885\n",
      "Accuracy on evaluation data: 8144 / 10000\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 989.0189330706221\n",
      "Accuracy on training data: 992 / 1000\n",
      "Cost on evaluation data: 990.0987387519884\n",
      "Accuracy on evaluation data: 8121 / 10000\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 980.7376104636594\n",
      "Accuracy on training data: 992 / 1000\n",
      "Cost on evaluation data: 981.8105416680027\n",
      "Accuracy on evaluation data: 8136 / 10000\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 972.535106294219\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 973.6177212408392\n",
      "Accuracy on evaluation data: 8135 / 10000\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 964.3684169276578\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 965.4517666487301\n",
      "Accuracy on evaluation data: 8163 / 10000\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 956.2993697189273\n",
      "Accuracy on training data: 995 / 1000\n",
      "Cost on evaluation data: 957.3926199504664\n",
      "Accuracy on evaluation data: 8177 / 10000\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 948.252120451752\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 949.3522467436115\n",
      "Accuracy on evaluation data: 8182 / 10000\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 940.2554095675058\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 941.3470458585681\n",
      "Accuracy on evaluation data: 8167 / 10000\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 932.3201771178805\n",
      "Accuracy on training data: 995 / 1000\n",
      "Cost on evaluation data: 933.4264645939137\n",
      "Accuracy on evaluation data: 8174 / 10000\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 924.4366150281878\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 925.5499291584887\n",
      "Accuracy on evaluation data: 8190 / 10000\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 916.6052185995777\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 917.7246925100491\n",
      "Accuracy on evaluation data: 8187 / 10000\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 908.8137212527303\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 909.9347081594067\n",
      "Accuracy on evaluation data: 8169 / 10000\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 901.0830642996261\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 902.2107310434396\n",
      "Accuracy on evaluation data: 8177 / 10000\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 893.4324548093516\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 894.5625241709088\n",
      "Accuracy on evaluation data: 8193 / 10000\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 885.8289052766596\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 886.9639638942211\n",
      "Accuracy on evaluation data: 8189 / 10000\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 878.2810117409441\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 879.4244219550106\n",
      "Accuracy on evaluation data: 8181 / 10000\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 870.789378078489\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 871.9281173083243\n",
      "Accuracy on evaluation data: 8197 / 10000\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 863.3586204436498\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 864.5083913779064\n",
      "Accuracy on evaluation data: 8191 / 10000\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 855.9861871488929\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 857.1331739352325\n",
      "Accuracy on evaluation data: 8201 / 10000\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 848.6416573387771\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 849.7974511447944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 8210 / 10000\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 841.3863603216412\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 842.5425609004092\n",
      "Accuracy on evaluation data: 8215 / 10000\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 834.1656690029707\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 835.3201247617696\n",
      "Accuracy on evaluation data: 8217 / 10000\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 827.0181541673811\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 828.1728889066409\n",
      "Accuracy on evaluation data: 8210 / 10000\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 819.9209271337588\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 821.0802181818418\n",
      "Accuracy on evaluation data: 8213 / 10000\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 812.8470099080919\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 813.9938741482866\n",
      "Accuracy on evaluation data: 8242 / 10000\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 805.8624784272\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 807.0129103597318\n",
      "Accuracy on evaluation data: 8221 / 10000\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 798.9211882169274\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 800.0834411933848\n",
      "Accuracy on evaluation data: 8225 / 10000\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 792.0347669592365\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 793.1978235304908\n",
      "Accuracy on evaluation data: 8228 / 10000\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 785.2142445083509\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 786.3735206205278\n",
      "Accuracy on evaluation data: 8241 / 10000\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 778.4361610328655\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 779.5879921516255\n",
      "Accuracy on evaluation data: 8248 / 10000\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 771.739719217371\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 772.9071143203531\n",
      "Accuracy on evaluation data: 8235 / 10000\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 765.077072572452\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 766.2423612006861\n",
      "Accuracy on evaluation data: 8233 / 10000\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 758.4629637814394\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 759.605471265212\n",
      "Accuracy on evaluation data: 8283 / 10000\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 751.9237498199259\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 753.0786601365022\n",
      "Accuracy on evaluation data: 8277 / 10000\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 745.4257310407625\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 746.5870855313568\n",
      "Accuracy on evaluation data: 8255 / 10000\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 738.9892513964681\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 740.142843977217\n",
      "Accuracy on evaluation data: 8270 / 10000\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 732.6175457620068\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 733.7743324911191\n",
      "Accuracy on evaluation data: 8271 / 10000\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 726.2967944274784\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 727.4404539192141\n",
      "Accuracy on evaluation data: 8280 / 10000\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 720.0155666563525\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 721.1651681961517\n",
      "Accuracy on evaluation data: 8283 / 10000\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 713.801734271125\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 714.9557941271761\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 707.6438075224462\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 708.7935008835527\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 701.5335594536189\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 702.6767097415577\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 695.4814922114427\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 696.6262370157684\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 689.481215263628\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 690.6274948849237\n",
      "Accuracy on evaluation data: 8295 / 10000\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 683.5393752252504\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 684.6862964775638\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 677.6449235509821\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 678.7958517758052\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 671.8046783110115\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 672.9492256502812\n",
      "Accuracy on evaluation data: 8317 / 10000\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 666.02081395067\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 667.163331604929\n",
      "Accuracy on evaluation data: 8310 / 10000\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 660.2773623650604\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 661.4183511154353\n",
      "Accuracy on evaluation data: 8326 / 10000\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 654.5940806075133\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 655.7369558832638\n",
      "Accuracy on evaluation data: 8318 / 10000\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 648.9667920903004\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 650.1089756142678\n",
      "Accuracy on evaluation data: 8317 / 10000\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 643.3868281010105\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 644.5239398844939\n",
      "Accuracy on evaluation data: 8322 / 10000\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 637.8541885467215\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 638.9901334852589\n",
      "Accuracy on evaluation data: 8326 / 10000\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 632.3641310701711\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 633.4963747636768\n",
      "Accuracy on evaluation data: 8341 / 10000\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 626.934583752324\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 628.0675242830475\n",
      "Accuracy on evaluation data: 8340 / 10000\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 621.5573393178028\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 622.6914578391476\n",
      "Accuracy on evaluation data: 8344 / 10000\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 616.2257938484086\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 617.3523515215546\n",
      "Accuracy on evaluation data: 8343 / 10000\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 610.9396223676349\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 612.0682281224753\n",
      "Accuracy on evaluation data: 8347 / 10000\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 605.7055834879266\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 606.8330724636422\n",
      "Accuracy on evaluation data: 8352 / 10000\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 600.5243572467833\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 601.6482474246036\n",
      "Accuracy on evaluation data: 8352 / 10000\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 595.3812989336274\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 596.5111678367163\n",
      "Accuracy on evaluation data: 8348 / 10000\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 590.289491795425\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 591.4166610232825\n",
      "Accuracy on evaluation data: 8354 / 10000\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 585.2485426853139\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 586.3727303547721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 8366 / 10000\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 580.2600893019332\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 581.3849466859306\n",
      "Accuracy on evaluation data: 8363 / 10000\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 575.2981506141005\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 576.4168247030275\n",
      "Accuracy on evaluation data: 8379 / 10000\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 570.4083533707984\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 571.5241873544395\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 565.5454827906468\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 566.6609192559854\n",
      "Accuracy on evaluation data: 8404 / 10000\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 560.7386619122908\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 561.8522309946244\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 555.9651274098733\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 557.070238442365\n",
      "Accuracy on evaluation data: 8408 / 10000\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 551.2447137475132\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 552.3513615225605\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 546.569972870895\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 547.6796530313705\n",
      "Accuracy on evaluation data: 8386 / 10000\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 541.9371353572177\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 543.0457442158918\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 537.3514100158627\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 538.4618855726894\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 532.8009520524387\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 533.9018702146017\n",
      "Accuracy on evaluation data: 8414 / 10000\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 528.3050677196879\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 529.4126921822447\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 523.8316833416515\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 524.9351106871205\n",
      "Accuracy on evaluation data: 8408 / 10000\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 519.4145418265741\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 520.5201174449481\n",
      "Accuracy on evaluation data: 8416 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1203.4062991530584,\n",
       "  1192.985413664471,\n",
       "  1182.9081248020473,\n",
       "  1173.4323235306697,\n",
       "  1164.2212612819574,\n",
       "  1155.165712549201,\n",
       "  1146.2867036503221,\n",
       "  1137.2833110812892,\n",
       "  1128.2588473238518,\n",
       "  1119.4047314673658,\n",
       "  1110.5355101167672,\n",
       "  1101.609957639381,\n",
       "  1092.8270754089729,\n",
       "  1084.0371431365063,\n",
       "  1075.2526908229027,\n",
       "  1066.5870980774855,\n",
       "  1057.889510794536,\n",
       "  1049.2802243071455,\n",
       "  1040.6797922046705,\n",
       "  1032.0985690518812,\n",
       "  1023.6026270906094,\n",
       "  1015.120677864256,\n",
       "  1006.7063537636782,\n",
       "  998.3809419712885,\n",
       "  990.0987387519884,\n",
       "  981.8105416680027,\n",
       "  973.6177212408392,\n",
       "  965.4517666487301,\n",
       "  957.3926199504664,\n",
       "  949.3522467436115,\n",
       "  941.3470458585681,\n",
       "  933.4264645939137,\n",
       "  925.5499291584887,\n",
       "  917.7246925100491,\n",
       "  909.9347081594067,\n",
       "  902.2107310434396,\n",
       "  894.5625241709088,\n",
       "  886.9639638942211,\n",
       "  879.4244219550106,\n",
       "  871.9281173083243,\n",
       "  864.5083913779064,\n",
       "  857.1331739352325,\n",
       "  849.7974511447944,\n",
       "  842.5425609004092,\n",
       "  835.3201247617696,\n",
       "  828.1728889066409,\n",
       "  821.0802181818418,\n",
       "  813.9938741482866,\n",
       "  807.0129103597318,\n",
       "  800.0834411933848,\n",
       "  793.1978235304908,\n",
       "  786.3735206205278,\n",
       "  779.5879921516255,\n",
       "  772.9071143203531,\n",
       "  766.2423612006861,\n",
       "  759.605471265212,\n",
       "  753.0786601365022,\n",
       "  746.5870855313568,\n",
       "  740.142843977217,\n",
       "  733.7743324911191,\n",
       "  727.4404539192141,\n",
       "  721.1651681961517,\n",
       "  714.9557941271761,\n",
       "  708.7935008835527,\n",
       "  702.6767097415577,\n",
       "  696.6262370157684,\n",
       "  690.6274948849237,\n",
       "  684.6862964775638,\n",
       "  678.7958517758052,\n",
       "  672.9492256502812,\n",
       "  667.163331604929,\n",
       "  661.4183511154353,\n",
       "  655.7369558832638,\n",
       "  650.1089756142678,\n",
       "  644.5239398844939,\n",
       "  638.9901334852589,\n",
       "  633.4963747636768,\n",
       "  628.0675242830475,\n",
       "  622.6914578391476,\n",
       "  617.3523515215546,\n",
       "  612.0682281224753,\n",
       "  606.8330724636422,\n",
       "  601.6482474246036,\n",
       "  596.5111678367163,\n",
       "  591.4166610232825,\n",
       "  586.3727303547721,\n",
       "  581.3849466859306,\n",
       "  576.4168247030275,\n",
       "  571.5241873544395,\n",
       "  566.6609192559854,\n",
       "  561.8522309946244,\n",
       "  557.070238442365,\n",
       "  552.3513615225605,\n",
       "  547.6796530313705,\n",
       "  543.0457442158918,\n",
       "  538.4618855726894,\n",
       "  533.9018702146017,\n",
       "  529.4126921822447,\n",
       "  524.9351106871205,\n",
       "  520.5201174449481],\n",
       " [5329,\n",
       "  5936,\n",
       "  6975,\n",
       "  7213,\n",
       "  7466,\n",
       "  7639,\n",
       "  7585,\n",
       "  7741,\n",
       "  7810,\n",
       "  7815,\n",
       "  7944,\n",
       "  7949,\n",
       "  7950,\n",
       "  7965,\n",
       "  7980,\n",
       "  8002,\n",
       "  8066,\n",
       "  8079,\n",
       "  8073,\n",
       "  8114,\n",
       "  8109,\n",
       "  8151,\n",
       "  8159,\n",
       "  8144,\n",
       "  8121,\n",
       "  8136,\n",
       "  8135,\n",
       "  8163,\n",
       "  8177,\n",
       "  8182,\n",
       "  8167,\n",
       "  8174,\n",
       "  8190,\n",
       "  8187,\n",
       "  8169,\n",
       "  8177,\n",
       "  8193,\n",
       "  8189,\n",
       "  8181,\n",
       "  8197,\n",
       "  8191,\n",
       "  8201,\n",
       "  8210,\n",
       "  8215,\n",
       "  8217,\n",
       "  8210,\n",
       "  8213,\n",
       "  8242,\n",
       "  8221,\n",
       "  8225,\n",
       "  8228,\n",
       "  8241,\n",
       "  8248,\n",
       "  8235,\n",
       "  8233,\n",
       "  8283,\n",
       "  8277,\n",
       "  8255,\n",
       "  8270,\n",
       "  8271,\n",
       "  8280,\n",
       "  8283,\n",
       "  8284,\n",
       "  8284,\n",
       "  8296,\n",
       "  8301,\n",
       "  8295,\n",
       "  8293,\n",
       "  8300,\n",
       "  8317,\n",
       "  8310,\n",
       "  8326,\n",
       "  8318,\n",
       "  8317,\n",
       "  8322,\n",
       "  8326,\n",
       "  8341,\n",
       "  8340,\n",
       "  8344,\n",
       "  8343,\n",
       "  8347,\n",
       "  8352,\n",
       "  8352,\n",
       "  8348,\n",
       "  8354,\n",
       "  8366,\n",
       "  8363,\n",
       "  8379,\n",
       "  8384,\n",
       "  8404,\n",
       "  8385,\n",
       "  8408,\n",
       "  8399,\n",
       "  8386,\n",
       "  8400,\n",
       "  8401,\n",
       "  8414,\n",
       "  8398,\n",
       "  8408,\n",
       "  8416],\n",
       " [1203.0090214904178,\n",
       "  1192.4582025147165,\n",
       "  1182.3362117293307,\n",
       "  1172.7878897212183,\n",
       "  1163.543130540585,\n",
       "  1154.457855789429,\n",
       "  1145.48987008091,\n",
       "  1136.4763475310592,\n",
       "  1127.4506072937318,\n",
       "  1118.5301572298756,\n",
       "  1109.6540165233446,\n",
       "  1100.7226382869826,\n",
       "  1091.898488197894,\n",
       "  1083.0925065382703,\n",
       "  1074.3047102031946,\n",
       "  1065.6137192106235,\n",
       "  1056.9217263347111,\n",
       "  1048.2854082230042,\n",
       "  1039.6791453171982,\n",
       "  1031.0915281055509,\n",
       "  1022.5742678820674,\n",
       "  1014.0890842007944,\n",
       "  1005.6748902372358,\n",
       "  997.324754855575,\n",
       "  989.0189330706221,\n",
       "  980.7376104636594,\n",
       "  972.535106294219,\n",
       "  964.3684169276578,\n",
       "  956.2993697189273,\n",
       "  948.252120451752,\n",
       "  940.2554095675058,\n",
       "  932.3201771178805,\n",
       "  924.4366150281878,\n",
       "  916.6052185995777,\n",
       "  908.8137212527303,\n",
       "  901.0830642996261,\n",
       "  893.4324548093516,\n",
       "  885.8289052766596,\n",
       "  878.2810117409441,\n",
       "  870.789378078489,\n",
       "  863.3586204436498,\n",
       "  855.9861871488929,\n",
       "  848.6416573387771,\n",
       "  841.3863603216412,\n",
       "  834.1656690029707,\n",
       "  827.0181541673811,\n",
       "  819.9209271337588,\n",
       "  812.8470099080919,\n",
       "  805.8624784272,\n",
       "  798.9211882169274,\n",
       "  792.0347669592365,\n",
       "  785.2142445083509,\n",
       "  778.4361610328655,\n",
       "  771.739719217371,\n",
       "  765.077072572452,\n",
       "  758.4629637814394,\n",
       "  751.9237498199259,\n",
       "  745.4257310407625,\n",
       "  738.9892513964681,\n",
       "  732.6175457620068,\n",
       "  726.2967944274784,\n",
       "  720.0155666563525,\n",
       "  713.801734271125,\n",
       "  707.6438075224462,\n",
       "  701.5335594536189,\n",
       "  695.4814922114427,\n",
       "  689.481215263628,\n",
       "  683.5393752252504,\n",
       "  677.6449235509821,\n",
       "  671.8046783110115,\n",
       "  666.02081395067,\n",
       "  660.2773623650604,\n",
       "  654.5940806075133,\n",
       "  648.9667920903004,\n",
       "  643.3868281010105,\n",
       "  637.8541885467215,\n",
       "  632.3641310701711,\n",
       "  626.934583752324,\n",
       "  621.5573393178028,\n",
       "  616.2257938484086,\n",
       "  610.9396223676349,\n",
       "  605.7055834879266,\n",
       "  600.5243572467833,\n",
       "  595.3812989336274,\n",
       "  590.289491795425,\n",
       "  585.2485426853139,\n",
       "  580.2600893019332,\n",
       "  575.2981506141005,\n",
       "  570.4083533707984,\n",
       "  565.5454827906468,\n",
       "  560.7386619122908,\n",
       "  555.9651274098733,\n",
       "  551.2447137475132,\n",
       "  546.569972870895,\n",
       "  541.9371353572177,\n",
       "  537.3514100158627,\n",
       "  532.8009520524387,\n",
       "  528.3050677196879,\n",
       "  523.8316833416515,\n",
       "  519.4145418265741],\n",
       " [638,\n",
       "  723,\n",
       "  837,\n",
       "  867,\n",
       "  904,\n",
       "  932,\n",
       "  940,\n",
       "  947,\n",
       "  955,\n",
       "  961,\n",
       "  963,\n",
       "  966,\n",
       "  973,\n",
       "  976,\n",
       "  976,\n",
       "  981,\n",
       "  982,\n",
       "  983,\n",
       "  983,\n",
       "  984,\n",
       "  987,\n",
       "  988,\n",
       "  990,\n",
       "  991,\n",
       "  992,\n",
       "  992,\n",
       "  993,\n",
       "  994,\n",
       "  995,\n",
       "  996,\n",
       "  996,\n",
       "  995,\n",
       "  996,\n",
       "  996,\n",
       "  996,\n",
       "  997,\n",
       "  997,\n",
       "  997,\n",
       "  998,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "import network2\n",
    "\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(list(training_data)[:1000], 100, 10, 0.5,  #list(training_data)[:1000]\n",
    "    evaluation_data=test_data,\n",
    "    lmbda = 0.1, # this is a regularization parameter\n",
    "    monitor_evaluation_cost=True,\n",
    "    monitor_evaluation_accuracy=True,\n",
    "    monitor_training_cost=True,\n",
    "    monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# network2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 激活函数\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# cost function\n",
    "class QuadraticCost(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \n",
    "        return 0.5*np.linalg.norm(a - y) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a - y)*sigmoid_prime(z)\n",
    "    \n",
    "class CrossEntropyCost(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \n",
    "        return np.sum(np.nan_to_num(-y * np.log(a) - (1-y))*np.log(1 - a))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a - y)\n",
    "\n",
    "\n",
    "# Main Network Class\n",
    "class Network2(object):\n",
    "    \n",
    "    def __init__(self, sizes, cost = CrossEntropyCost):\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost = cost\n",
    "    \n",
    "    def default_weight_initializer(self):\n",
    "        \n",
    "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) / np.sqrt(x)\n",
    "                       for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "    \n",
    "    def large_weight_initializer(self):\n",
    "\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "           lmbda =0.0,\n",
    "           evaluation_data = None,\n",
    "           monitor_evaluation_cost = False,\n",
    "           monitor_evaluation_accuracy = False,\n",
    "           monitor_training_cost = False,\n",
    "           monitor_training_accuracy = False,\n",
    "           early_stopping_n = 0):\n",
    "        \n",
    "#         best_accuracy = 1\n",
    "        \n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if evaluation_data:\n",
    "            evaluation_data = list(evaluation_data)\n",
    "            n_data = len(evaluation_data)\n",
    "        \n",
    "        # early stopping functionality:\n",
    "        best_accuracy = 0\n",
    "        no_accuracy_change = 0\n",
    "        \n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                    training_data[k: k + mini_batch_size]\n",
    "                    for k in range(0, n, mini_batch_size)]\n",
    "            print(\"Epoch %s training complete\" %j)\n",
    "\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost))\n",
    "\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
    "\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))  \n",
    "\n",
    "            # Early stopping:\n",
    "\n",
    "            if early_stopping_n > 0:\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    no_accuracy_change = 0\n",
    "                else:\n",
    "                    no_accuracy_change +=1\n",
    "                if (no_accuracy_change == early_stopping_n):\n",
    "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1 - eta * (lmbda / n)) * w - (eta / len(mini_batch)) * nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(mini_batch)) * nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]  # list to store all the activations, layer by layer\n",
    "        zs = []  # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    \n",
    "    def accuracy(self, data, convert=False):\n",
    "\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                       for (x, y) in data]\n",
    "\n",
    "        result_accuracy = sum(int(x == y) for (x, y) in results)\n",
    "        return result_accuracy\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y) / len(data)\n",
    "            cost += 0.5 * (lmbda / len(data)) * sum(\n",
    "                np.linalg.norm(w) ** 2 for w in self.weights)  # '**' - to the power of.\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- L2 regularization\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "C&=&C_0 + \\frac{\\lambda}{2n} \\sum_w w^2\\\\\n",
    "w \\rightarrow w' &= &w\\left(1 - \\frac{\\eta \\lambda}{n} \\right)\n",
    "  - \\eta \\frac{\\partial C_0}{\\partial w}.\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-581789e6e931>:4: RuntimeWarning: divide by zero encountered in log\n",
      "  np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
      "<ipython-input-5-581789e6e931>:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = 0.0\n",
    "y = 0.0\n",
    "np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 3940.07802519372\n",
      "Accuracy on training data: 47050 / 50000\n",
      "Cost on evaluation data: 3940.074064955377\n",
      "Accuracy on evaluation data: 9432 / 10000\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 4959.085340145698\n",
      "Accuracy on training data: 47528 / 50000\n",
      "Cost on evaluation data: 4959.0930469967125\n",
      "Accuracy on evaluation data: 9505 / 10000\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 5465.865270923692\n",
      "Accuracy on training data: 47570 / 50000\n",
      "Cost on evaluation data: 5465.8812296838005\n",
      "Accuracy on evaluation data: 9489 / 10000\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 5798.892292840804\n",
      "Accuracy on training data: 47952 / 50000\n",
      "Cost on evaluation data: 5798.912051636081\n",
      "Accuracy on evaluation data: 9572 / 10000\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 5991.915278811805\n",
      "Accuracy on training data: 48050 / 50000\n",
      "Cost on evaluation data: 5991.931314398319\n",
      "Accuracy on evaluation data: 9581 / 10000\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 6087.043429676771\n",
      "Accuracy on training data: 48175 / 50000\n",
      "Cost on evaluation data: 6087.08302080189\n",
      "Accuracy on evaluation data: 9596 / 10000\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 6238.65300292026\n",
      "Accuracy on training data: 48050 / 50000\n",
      "Cost on evaluation data: 6238.681020397182\n",
      "Accuracy on evaluation data: 9565 / 10000\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 6326.353278146069\n",
      "Accuracy on training data: 48161 / 50000\n",
      "Cost on evaluation data: 6326.3776089789735\n",
      "Accuracy on evaluation data: 9580 / 10000\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 6391.342880471811\n",
      "Accuracy on training data: 48287 / 50000\n",
      "Cost on evaluation data: 6391.373205896668\n",
      "Accuracy on evaluation data: 9621 / 10000\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 6393.74456043591\n",
      "Accuracy on training data: 48360 / 50000\n",
      "Cost on evaluation data: 6393.7774219747325\n",
      "Accuracy on evaluation data: 9590 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([3940.074064955377,\n",
       "  4959.0930469967125,\n",
       "  5465.8812296838005,\n",
       "  5798.912051636081,\n",
       "  5991.931314398319,\n",
       "  6087.08302080189,\n",
       "  6238.681020397182,\n",
       "  6326.3776089789735,\n",
       "  6391.373205896668,\n",
       "  6393.7774219747325],\n",
       " [9432, 9505, 9489, 9572, 9581, 9596, 9565, 9580, 9621, 9590],\n",
       " [3940.07802519372,\n",
       "  4959.085340145698,\n",
       "  5465.865270923692,\n",
       "  5798.892292840804,\n",
       "  5991.915278811805,\n",
       "  6087.043429676771,\n",
       "  6238.65300292026,\n",
       "  6326.353278146069,\n",
       "  6391.342880471811,\n",
       "  6393.74456043591],\n",
       " [47050, 47528, 47570, 47952, 48050, 48175, 48050, 48161, 48287, 48360])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = \\\n",
    "mnist_loader.load_data_wrapper()\n",
    "import network2\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.SGD(training_data, 10, 10, 0.5,\n",
    "        lmbda = 5.0,\n",
    "        evaluation_data=validation_data,\n",
    "        monitor_evaluation_accuracy=True,\n",
    "        monitor_evaluation_cost=True,\n",
    "        monitor_training_accuracy=True,\n",
    "        monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Epoch 0: validation accuracy 92.45%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 91.74%\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Epoch 1: validation accuracy 94.53%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 93.72%\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Epoch 2: validation accuracy 95.64%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 95.02%\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Epoch 3: validation accuracy 96.22%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 95.69%\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 4: validation accuracy 96.67%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.14%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Epoch 5: validation accuracy 96.92%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.52%\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Epoch 6: validation accuracy 97.02%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.81%\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Epoch 7: validation accuracy 97.17%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.99%\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Epoch 8: validation accuracy 97.30%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.20%\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 9: validation accuracy 97.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 97.25%\n",
      "Finished training network.\n",
      "Best validation accuracy of 97.34% obtained at iteration 49999\n",
      "Corresponding test accuracy of 97.25%\n"
     ]
    }
   ],
   "source": [
    "# - network3.py example:\n",
    "import network3\n",
    "from network3 import Network, ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer # softmax plus log-likelihood cost is more common in modern image classification networks.\n",
    "\n",
    "# read data:\n",
    "training_data, validation_data, test_data = network3.load_data_shared()\n",
    "# mini-batch size:\n",
    "mini_batch_size = 10\n",
    "net = Network([\n",
    "    FullyConnectedLayer(n_in=784, n_out=100),\n",
    "    SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
    "net.SGD(training_data, 10, mini_batch_size, 0.1, validation_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Compiler\\Anaconda3\\Lib\\site-packages\\theano\\tensor\\nnet\\conv.py:98: UserWarning: theano.tensor.nnet.conv.conv2d is deprecated. Use theano.tensor.nnet.conv2d instead.\n",
      "  warnings.warn(\"theano.tensor.nnet.conv.conv2d is deprecated.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2cba7a313c4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mFullyConnectedLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_in\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Study\\COURSE\\neural networks and deep learning\\neural-network\\notebook_by_lin\\network3.py\u001b[0m in \u001b[0;36mSGD\u001b[1;34m(self, training_data, epochs, mini_batch_size, eta, validation_data, test_data, lmbda)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training mini-batch number {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mcost_ij\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnum_training_batches\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     validation_accuracy = np.mean(\n",
      "\u001b[1;32mD:\\Compiler\\Anaconda3\\Lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Network([\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
    "                  filter_shape=(20, 1, 5, 5),\n",
    "                  poolsize=(2, 2)),\n",
    "    FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
    "    SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
    "net.SGD(training_data, 10, mini_batch_size, 0.1, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
